---
title: "Homework 6"
author: "Sarah Younes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

As always, I will begin by loading necessary packages. Additionally, I will set my seed for reproducibility.

```{r message = FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
set.seed(1)
```

## Problem 1

First, I will import and clean the data, including filtering the data to necessary items.

```{r homicide data}
homicide_data =
  read.csv(
    "./data/homicide-data.csv",
    na = "Unknown") |>
  janitor::clean_names() |>
  drop_na() |>
  mutate(
    victim_last = str_to_title(victim_last),
    victim_first = str_to_title(victim_first),
    city_state = paste(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    status = case_when(
      disposition == "Closed without arrest" ~ "Solved",
      disposition == "Closed by arrest" ~ "Solved",
      disposition == "Open/No arrest" ~ "Unsolved")) |>
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, OK"))) |>
  filter(victim_race == "White" | victim_race == "Black")
```

Now, I will fit a logistic regression for Baltimore, MD with `status` as the outcome and `victim_age`, `victim_sex`, and `victim_race` as predictors. I will tidy the output and obtain an estimate of the odds ratio.

```{r logistic regression}
regression_data =
  homicide_data |>
  mutate(
    status = case_when(
      status == "Solved" ~ 1,
      status == "Unsolved" ~ 0))

status_fit =
  glm(status ~ victim_age + victim_sex + victim_race, data = regression_data, family = "binomial") |>
  broom::tidy() |>
  mutate(OR = exp(estimate))

status_fit
```

## Problem 2

First, I will download the cleaned weather data set as instructed.

```{r weather data, message = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

I will now create a simple linear regression with `tmax` as the outcome and `tmin` and `prcp` as the predictors.

```{r linear regression}
tmax_fit = lm(tmax ~ tmin + prcp, data = weather_df)

tmax_fit
```

Next, I will draw 5,000 bootstrap samples and produce estimates of r-squared using `broom::glance`.

```{r r2 bootstrap}
r2_bootstrap =
  weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, \(weather_df) lm(tmax ~ tmin + prcp, data = weather_df)),
    results = map(models, broom::glance),
    sample_number = row_number()) |>
  unnest(results) |>
  select(sample_number, r.squared)

r2_bootstrap
```

I will draw 5,000 bootstrap samples and produce estimates of log(β̂ 1∗β̂ 2) using `broom::tidy()` and `pivot_wider`.

```{r log bootstrap, message = FALSE}
log_bootstrap =
  weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, \(weather_df) lm(tmax ~ tmin + prcp, data = weather_df)),
    results = map(models, broom::tidy),
    sample_number = row_number()) |>
  unnest(results) |>
  filter(term %in% c("prcp", "tmin")) |>
  pivot_wider(
    id_cols = sample_number,
    names_from = term,
    values_from = estimate) |>
  mutate(
    log = log(tmin * prcp))

log_bootstrap
```

Since ∗β̂ 2  is negative, `r sum(is.na(pull(log_bootstrap, log)))/5000*100` % of estimates of log from the bootstrap samples were negative and thus returned NaN.

Now, I will plot the distribution of the estimates of r-squared and log(β̂ 1∗β̂ 2) from the bootstrap samples.

```{r distributions}
r2_bootstrap |>
  ggplot(aes(x = r.squared)) + geom_density()

log_bootstrap |>
  ggplot(aes(x = log)) + geom_density()
```

The density plot for r-squared shows that the peak is around the mean, which is `r r2_bootstrap |> summarize(mean = mean(r.squared))`. The minimum is `r r2_bootstrap |> min(pull(r2_bootstrap, r.squared))` and the maximum is `r r2_bootstrap |> max(pull(r2_bootstrap, r.squared))`. These r-squared values indicate that the model (in other words, minimum temperature and average precipitation) predicts over `r (r2_bootstrap |> min(pull(r2_bootstrap, r.squared)))*100`% and an average of `r r2_bootstrap |> summarize(mean = mean(r.squared))` of the variability in maximum temperature. This model is a great fit.

Finally, I will identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r-squared and log(β̂ 1∗β̂ 2). However, I dropped negative log values from the bootstrap sample to construct confidence intervals for estimates that are not missing.

```{r confidence intervals}
r2_bootstrap_ci =
  r2_bootstrap |>
  mutate(
    ci_lower = quantile(r.squared, 0.025),
    ci_upper = quantile(r.squared, 0.975))

r2_bootstrap_ci

log_bootstrap_ci =
  log_bootstrap |>
  drop_na(log) |>
  mutate(
    ci_lower = quantile(log, 0.025, na.rm = TRUE),
    ci_upper = quantile(log, 0.975, na.rm = TRUE))

log_bootstrap_ci
```

## Problem 3

I will import and clean the birthweight data set. Data cleaning involves removing "Unknown" race values and converting numeric values to factors.

```{r birthweight cleaning, message = FALSE}
birthweight_df =
  read_csv("./data/birthweight.csv") |>
  filter(frace != 9) |>
  mutate(
    frace = as.factor(frace),
    mrace = as.factor(mrace))

## I tried this code but it messed up my t-test: mutate(babysex = case_match(babysex, 1 ~ "male", 2 ~ "female"), malform = case_match(malform, 0 ~ "absent", 1 ~ "present"))
```

I will use a combination of a data-driven and theory/hypothesis-driven modeling process to propose a regression model for birthweight.

I will use run statistical tests to look at the relationship between the continuous outcome, `bwt`, and each independent variable. However, I am automatically excluding `pnumlbw` and `pnumsga` as potential independent variables because their values across the entire sample were all 0, so there cannot be any trends between them and `bwt`.

For continuous independent variables, I will run Pearson's correlation using `cor.test`; for binary categorical independent variables, I will run independent samples t-tests using `t.test`; and for categorical independent variables with three or more values, I will run ANOVA using `aov`.

Here are the steps I will take and criteria I will use:

1. I will run the statistical tests.
2. I will only consider p-values that are statistically significant at p<0.05.
3. I will use the following criteria to narrow down my independent variables:

     * For statistically significant continuous independent variables (tested with Pearson's correlation): the correlation coefficient estimate must be >0.7 to indicate a strong correlation.
     * For statistically significant categorical independent variables (tested with independent samples t-tests and ANOVA): I will look at the existing literature online to determine have the strongest relationships AND there must be enough values of all the categories in the birthweight data set to notice differences in birthweight by category.

I will include the variables with the strongest numeric and/or theoretical relationships in my model based on the criteria above.

Here are my statistical tests:

```{r statistical tests}
cor.test(pull(birthweight_df, bwt), pull(birthweight_df, bhead), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, blength), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, delwt), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, fincome), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, gaweeks), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, menarche), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, mheight), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, momage), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, parity), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, ppbmi), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, ppwt), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, smoken), method = "pearson") |> broom::tidy()

cor.test(pull(birthweight_df, bwt), pull(birthweight_df, wtgain), method = "pearson") |> broom::tidy()

t.test(pull(birthweight_df, bwt), pull(birthweight_df, babysex)) |> broom::tidy()

t.test(pull(birthweight_df, bwt), pull(birthweight_df, malform)) |> broom::tidy()

aov(birthweight_df$bwt ~ birthweight_df$frace) |> broom::tidy()

aov(birthweight_df$bwt ~ birthweight_df$mrace) |> broom::tidy()
```

The following variables showed both high visual relationships from ggplots and high statistical relationships (correlations above r-squared = 0.7): `bhead`,  `blength`, `babysex`, and `mrace`.

```{r birthweight model}
bwt_model = lm(bwt ~ bhead + blength + babysex + mrace, data = birthweight_df)
```

I will plot the residuals:

```{r}
birthweight_df |>
 modelr::add_residuals(bwt_model)

birthweight_df |>
  modelr::add_residuals(bwt_model) |>
  ggplot(aes(x = bwt, y = resid)) + geom_point() + geom_smooth()
```

I will plot the fitted values:

```{r}
modelr::add_predictions(birthweight_df, bwt_model)

birthweight_df |>
  modelr::add_predictions(bwt_model) |>
  ggplot(aes(x = bwt, y = pred)) + geom_point() + geom_smooth()
```

## delete the below

```{r}
resid_pred =
  birthweight_df |>
  modelr::add_predictions(bwt_model) |>
  modelr::add_residuals(bwt_model) |>
  select(bwt, resid, pred, everything())

resid_pred |>
  ggplot(aes(x = bwt)) +
  geom_point(aes(y = resid), color = "purple", alpha = 0.05) +
  geom_smooth(aes(y = resid), color = "purple") +
  geom_point(aes(y = pred), color = "blue", alpha = 0.05) +
  geom_smooth(aes(y = pred), color = "blue")
```

Now, I will compare my model to two others:

```{r comparison models}
option_1 = lm(bwt ~ blength + gaweeks, data = birthweight_df)

option_2 = lm(bwt ~ bhead + blength + babysex + (bhead * blength * babysex), data = birthweight_df)
```

```{r}
comparison_df = 
  crossv_mc(birthweight_df, 100)

comparison_df =
  comparison_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

comparison_df = 
  comparison_df |> 
  mutate(
    hypothesized = map(train, \(df) lm(bwt ~ bhead + blength + babysex + mrace, data = birthweight_df)),
    option_1 = map(train, \(df) lm(bwt ~ blength + gaweeks, data = birthweight_df)),
    option_2 = map(train, \(df) lm(bwt ~ bhead + blength + babysex + (bhead * blength * babysex), data = birthweight_df))) |>
  mutate(
    rmse_hypothesized = map2_dbl(hypothesized, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_option_1 = map2_dbl(option_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_option_2 = map2_dbl(option_2, test, \(mod, df) rmse(model = mod, data = df)))

comparison_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```